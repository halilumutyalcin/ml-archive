{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n #   for filename in filenames:\n  #      print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-20T19:55:04.594621Z","iopub.execute_input":"2022-06-20T19:55:04.594885Z","iopub.status.idle":"2022-06-20T19:55:04.599765Z","shell.execute_reply.started":"2022-06-20T19:55:04.594857Z","shell.execute_reply":"2022-06-20T19:55:04.598844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading the Data\nImporting all the necessary libraries required to run the following code for image colourization.","metadata":{}},{"cell_type":"code","source":"from torchvision.datasets import ImageFolder\nimport torchvision.transforms as T\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data.dataset import random_split\nimport torch.nn.functional as F\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt\nfrom skimage.color import rgb2lab, lab2rgb, rgb2gray","metadata":{"execution":{"iopub.status.busy":"2022-06-20T19:55:14.185467Z","iopub.execute_input":"2022-06-20T19:55:14.1858Z","iopub.status.idle":"2022-06-20T19:55:15.839213Z","shell.execute_reply.started":"2022-06-20T19:55:14.185769Z","shell.execute_reply":"2022-06-20T19:55:15.838461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Defining the directory for the dataset to be used. The dataset used here is a small 6 GB snippet of the ImageNet dataset. It has 2 classes: 'train' and 'val', containing 45000 and 5000 images respectively.","metadata":{}},{"cell_type":"code","source":"DATA_DIR = '../input/imagenet/imagenet/'","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2022-06-20T19:55:17.290147Z","iopub.execute_input":"2022-06-20T19:55:17.290483Z","iopub.status.idle":"2022-06-20T19:55:17.294871Z","shell.execute_reply.started":"2022-06-20T19:55:17.29045Z","shell.execute_reply":"2022-06-20T19:55:17.293438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Loading images contained in the data directory into a avriable called dataset using ImageFolder, and applying two transforms to all the images:\n1. Resizing the non-uniformly sized images into 256x256 images.\n2. Converting them into tensors.","metadata":{}},{"cell_type":"code","source":"dataset = ImageFolder(DATA_DIR, transform=T.Compose([T.Resize((256, 256)),T.ToTensor()]))","metadata":{"execution":{"iopub.status.busy":"2022-06-20T19:55:18.490061Z","iopub.execute_input":"2022-06-20T19:55:18.490389Z","iopub.status.idle":"2022-06-20T19:56:16.782104Z","shell.execute_reply.started":"2022-06-20T19:55:18.49034Z","shell.execute_reply":"2022-06-20T19:56:16.781252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As mentioned above, the dataset has a total 50000 images.","metadata":{}},{"cell_type":"code","source":"len(dataset)","metadata":{"execution":{"iopub.status.busy":"2022-05-21T09:34:52.556634Z","iopub.execute_input":"2022-05-21T09:34:52.556925Z","iopub.status.idle":"2022-05-21T09:34:52.566927Z","shell.execute_reply.started":"2022-05-21T09:34:52.556897Z","shell.execute_reply":"2022-05-21T09:34:52.565883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Creating the training and validation sets. A seed 42 is set to have the same training and validation datasets each time the notebook is run, and the two datasets are split using the random_split function from the pytorch libraries.","metadata":{}},{"cell_type":"code","source":"random_seed = 42\ntorch.manual_seed(random_seed)\n\nval_size = 1000\ntrain_size = len(dataset) - val_size\n\ntrain_ds, val_ds = random_split(dataset, [train_size, val_size])\nlen(train_ds), len(val_ds)","metadata":{"execution":{"iopub.status.busy":"2022-05-21T09:34:52.568719Z","iopub.execute_input":"2022-05-21T09:34:52.569237Z","iopub.status.idle":"2022-05-21T09:34:52.591848Z","shell.execute_reply.started":"2022-05-21T09:34:52.569199Z","shell.execute_reply":"2022-05-21T09:34:52.591074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The batch size is set to be 128.","metadata":{}},{"cell_type":"code","source":"batch_size = 128","metadata":{"execution":{"iopub.status.busy":"2022-05-21T09:34:52.594558Z","iopub.execute_input":"2022-05-21T09:34:52.594801Z","iopub.status.idle":"2022-05-21T09:34:52.600893Z","shell.execute_reply.started":"2022-05-21T09:34:52.594777Z","shell.execute_reply":"2022-05-21T09:34:52.599966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Loading the training and validation datasets into the CPU using DataLoader.","metadata":{}},{"cell_type":"code","source":"train_loader = DataLoader(train_ds, batch_size = batch_size, shuffle = True, num_workers = 4, pin_memory = True)\nval_loader = DataLoader(val_ds, batch_size = batch_size, num_workers = 4, pin_memory = True)","metadata":{"execution":{"iopub.status.busy":"2022-05-21T09:34:52.602334Z","iopub.execute_input":"2022-05-21T09:34:52.602912Z","iopub.status.idle":"2022-05-21T09:34:52.608839Z","shell.execute_reply.started":"2022-05-21T09:34:52.602871Z","shell.execute_reply":"2022-05-21T09:34:52.607967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Make cuda(GPU) the device if availability permits.","metadata":{}},{"cell_type":"code","source":"device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\ndevice","metadata":{"execution":{"iopub.status.busy":"2022-05-21T09:34:52.610449Z","iopub.execute_input":"2022-05-21T09:34:52.610986Z","iopub.status.idle":"2022-05-21T09:34:52.691901Z","shell.execute_reply.started":"2022-05-21T09:34:52.610942Z","shell.execute_reply":"2022-05-21T09:34:52.691205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Define a function to load data into the device assigned above.","metadata":{}},{"cell_type":"code","source":"def to_device(data, device):\n    if isinstance(data, (list, tuple)):\n        return [to_device(x, device) for x in data]\n    return data.to(device, non_blocking = True)","metadata":{"execution":{"iopub.status.busy":"2022-05-21T09:34:52.694041Z","iopub.execute_input":"2022-05-21T09:34:52.694697Z","iopub.status.idle":"2022-05-21T09:34:52.700931Z","shell.execute_reply.started":"2022-05-21T09:34:52.694659Z","shell.execute_reply":"2022-05-21T09:34:52.7002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Define a class to load data into a GPU, if it's available, otherwise into a CPU.","metadata":{}},{"cell_type":"code","source":"class DeviceDataLoader():\n    def __init__(self, dl, device):\n        self.dl = dl\n        self.device = device\n\n    def __iter__(self):\n        for batch in self.dl:\n            yield to_device(batch, self.device)\n  \n    def __len__(self):\n        return len(self.dl)","metadata":{"execution":{"iopub.status.busy":"2022-05-21T09:34:52.702412Z","iopub.execute_input":"2022-05-21T09:34:52.703071Z","iopub.status.idle":"2022-05-21T09:34:52.713796Z","shell.execute_reply.started":"2022-05-21T09:34:52.703034Z","shell.execute_reply":"2022-05-21T09:34:52.71289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since we are using a GPU, load the training and validation datasets into cuda(the selected device).","metadata":{}},{"cell_type":"code","source":"train_loader = DeviceDataLoader(train_loader, device)\nval_loader = DeviceDataLoader(val_loader, device)","metadata":{"execution":{"iopub.status.busy":"2022-05-21T09:34:52.715112Z","iopub.execute_input":"2022-05-21T09:34:52.715673Z","iopub.status.idle":"2022-05-21T09:34:52.723374Z","shell.execute_reply.started":"2022-05-21T09:34:52.715635Z","shell.execute_reply":"2022-05-21T09:34:52.722413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Defining the Model\nThe images are originally using the RGB colormodel. But for the purpose of image coval_loadertion, they have to be converted to Lab. The CIELAB is a colorspace which has 3 channels: L, a, and b. The function defined below converts images from RGB to CIELAB, and then splits the L and ab channels into the variables X and Y respectively. The range of ab channels is from -128 to 127, hence, the ab channels are normalized by dividing it by 128. The function returns finally returns X and Y loaded into cuda.","metadata":{}},{"cell_type":"code","source":"def generate_l_ab(images): \n    lab = rgb2lab(images.permute(0, 2, 3, 1).cpu().numpy())\n    X = lab[:,:,:,0]\n    X = X.reshape(X.shape+(1,))\n    Y = lab[:,:,:,1:] / 128\n    return to_device(torch.tensor(X, dtype = torch.float).permute(0, 3, 1, 2), device),to_device(torch.tensor(Y, dtype = torch.float).permute(0, 3, 1, 2), device)","metadata":{"execution":{"iopub.status.busy":"2022-05-21T09:34:52.724726Z","iopub.execute_input":"2022-05-21T09:34:52.725284Z","iopub.status.idle":"2022-05-21T09:34:52.73435Z","shell.execute_reply.started":"2022-05-21T09:34:52.725247Z","shell.execute_reply":"2022-05-21T09:34:52.733517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The Base Model class is defined as containing three functions. \n1. The *training_batch* function takes the batch of 128 images as input, generates the values of L and ab channels using the *generate_l_ab* function, gets the predicted ab channels using the forward function of the model, and calculates the MSE Loss between the actual and predicted ab channels.\n2. The *validation_batch* functions performs the same task as the *training_batch* function, except for the images in the validation dataset, which is evident from the function names.\n3. The function *validation_end_epoch* returns the average loss on the validation dataset.","metadata":{}},{"cell_type":"code","source":"class BaseModel(nn.Module):\n    def training_batch(self, batch):\n        images, _ = batch\n        X, Y = generate_l_ab(images)\n        outputs = self.forward(X)\n        loss = F.mse_loss(outputs, Y)\n        return loss\n\n    def validation_batch(self, batch):\n        images, _ = batch\n        X, Y = generate_l_ab(images)\n        outputs = self.forward(X)\n        loss = F.mse_loss(outputs, Y)\n        return {'val_loss' : loss.item()}\n\n    def validation_end_epoch(self, outputs):\n        epoch_loss = sum([x['val_loss'] for x in outputs]) / len(outputs)\n        return {'epoch_loss' : epoch_loss}","metadata":{"execution":{"iopub.status.busy":"2022-05-21T09:34:52.737164Z","iopub.execute_input":"2022-05-21T09:34:52.737442Z","iopub.status.idle":"2022-05-21T09:34:52.747931Z","shell.execute_reply.started":"2022-05-21T09:34:52.737415Z","shell.execute_reply":"2022-05-21T09:34:52.747227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A helper function is defined to get the appropriate padding in order to keep the size of the output image same as the input image during convolution.","metadata":{}},{"cell_type":"code","source":"def get_padding(kernel_size: int, stride: int = 1, dilation: int = 1, **_) -> int:\n    padding = ((stride - 1) + dilation * (kernel_size - 1)) // 2\n    return padding","metadata":{"execution":{"iopub.status.busy":"2022-05-21T09:34:52.74921Z","iopub.execute_input":"2022-05-21T09:34:52.749632Z","iopub.status.idle":"2022-05-21T09:34:52.757727Z","shell.execute_reply.started":"2022-05-21T09:34:52.749596Z","shell.execute_reply":"2022-05-21T09:34:52.756971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The *Encoder_Decoder* class is an extension of the Base Model and it contains the code for the architecture of the model.","metadata":{}},{"cell_type":"code","source":"class Encoder_Decoder(BaseModel):\n    def __init__(self):\n        super().__init__()\n        self.network = nn.Sequential(\n            nn.Conv2d(1, 64, kernel_size = 3, stride = 2, padding = get_padding(3, 2)),\n            nn.ReLU(),\n            nn.BatchNorm2d(64),\n            nn.Conv2d(64, 128, kernel_size = 3, padding = get_padding(3)),\n            nn.ReLU(),\n            nn.BatchNorm2d(128),\n        \n            nn.Conv2d(128, 128, kernel_size = 3, stride = 2, padding = get_padding(3, 2)),\n            nn.ReLU(),\n            nn.BatchNorm2d(128),\n            nn.Conv2d(128, 256, kernel_size = 3, padding = get_padding(3)),\n            nn.ReLU(),\n            nn.BatchNorm2d(256),\n        \n            nn.Conv2d(256, 256, kernel_size = 3, stride = 2, padding = get_padding(3, 2)),\n            nn.ReLU(),\n            nn.BatchNorm2d(256),\n            nn.Conv2d(256, 512, kernel_size = 3, padding = get_padding(3)),\n            nn.ReLU(),\n            nn.BatchNorm2d(512),\n            \n            nn.Conv2d(512, 512, kernel_size = 3, padding = get_padding(3)),\n            nn.ReLU(),\n            nn.BatchNorm2d(512),\n            nn.Conv2d(512, 256, kernel_size = 3, padding = get_padding(3)),\n            nn.ReLU(),\n            nn.BatchNorm2d(256),\n        \n            nn.Conv2d(256, 128, kernel_size = 3, padding = get_padding(3)),\n            nn.Upsample(size = (64,64)),\n            nn.Conv2d(128, 64, kernel_size = 3, padding = get_padding(3)),\n            nn.Upsample(size = (128,128)),\n            nn.Conv2d(64, 32, kernel_size = 3, padding = get_padding(3)),\n            nn.Conv2d(32, 16, kernel_size = 3, padding = get_padding(3)),\n            nn.Conv2d(16, 2, kernel_size = 3, padding = get_padding(3)),\n            nn.Tanh(),\n            nn.Upsample(size = (256,256))\n    )\n\n    def forward(self, images):\n        return self.network(images)     \n    ","metadata":{"execution":{"iopub.status.busy":"2022-05-21T09:34:52.760823Z","iopub.execute_input":"2022-05-21T09:34:52.761063Z","iopub.status.idle":"2022-05-21T09:34:52.780001Z","shell.execute_reply.started":"2022-05-21T09:34:52.76104Z","shell.execute_reply":"2022-05-21T09:34:52.779363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The model is defined and loaded to cuda.","metadata":{}},{"cell_type":"code","source":"model = Encoder_Decoder()\nto_device(model, device)","metadata":{"execution":{"iopub.status.busy":"2022-05-21T09:34:52.781164Z","iopub.execute_input":"2022-05-21T09:34:52.781907Z","iopub.status.idle":"2022-05-21T09:34:56.127939Z","shell.execute_reply.started":"2022-05-21T09:34:52.78187Z","shell.execute_reply":"2022-05-21T09:34:56.127165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training the Model\nThe *validate* and *fit* functions are defined to keep track of the loss and train the model.","metadata":{}},{"cell_type":"code","source":"@torch.no_grad()\ndef validate(model, val_loader):\n    model.eval()\n    outputs = [model.validation_batch(batch) for batch in val_loader]\n    return model.validation_end_epoch(outputs)\n\ndef fit(model, epochs, learning_rate, train_loader, val_loader, optimization_func = torch.optim.SGD):\n    torch.cuda.empty_cache()\n    history = []\n    optimizer = optimization_func(model.parameters(), learning_rate)\n    for epoch in range(epochs):\n        train_losses = []\n        model.train()\n        for batch in tqdm(train_loader):\n            loss = model.training_batch(batch)\n            train_losses.append(loss)\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n        result = validate(model, val_loader)\n        result['train_loss'] = torch.stack(train_losses).mean().item()\n        history.append(result)\n        print('Epoch: {}, Train loss: {:.4f}, Validation loss: {:.4f}'.format(epoch, result['train_loss'], result['epoch_loss']))\n    return history","metadata":{"execution":{"iopub.status.busy":"2022-05-21T09:34:56.129108Z","iopub.execute_input":"2022-05-21T09:34:56.129531Z","iopub.status.idle":"2022-05-21T09:34:56.139901Z","shell.execute_reply.started":"2022-05-21T09:34:56.1295Z","shell.execute_reply":"2022-05-21T09:34:56.138982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Initializing the loss.","metadata":{}},{"cell_type":"code","source":"history = [validate(model, val_loader)]\nhistory\n","metadata":{"execution":{"iopub.status.busy":"2022-05-21T09:34:56.144851Z","iopub.execute_input":"2022-05-21T09:34:56.145604Z","iopub.status.idle":"2022-05-21T09:35:23.20595Z","shell.execute_reply.started":"2022-05-21T09:34:56.14555Z","shell.execute_reply":"2022-05-21T09:35:23.205205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Training.","metadata":{}},{"cell_type":"code","source":"history += fit(model, 5, 0.001, train_loader, val_loader, torch.optim.Adam)","metadata":{"execution":{"iopub.status.busy":"2022-05-21T09:35:23.207509Z","iopub.execute_input":"2022-05-21T09:35:23.207806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Saving the model to be able to continue training the model from the same point.","metadata":{}},{"cell_type":"code","source":"checkpoint = {'model': Encoder_Decoder(),\n              'state_dict': model.state_dict()}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(checkpoint, 'test.pth')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Loading the pth file to continue training from the same parameters onwards.","metadata":{}},{"cell_type":"code","source":"def load_checkpoint(filepath):\n    checkpoint = torch.load(filepath)\n    model = checkpoint['model']\n    model.load_state_dict(checkpoint['state_dict'])\n    for parameter in model.parameters():\n        parameter.requires_grad = True\n    \n    model.eval()\n    \n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = load_checkpoint('../input/testing/test21.pth')\nprint(model)\nto_device(model, device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Testing the Model\nCombining the L channel and the predicted ab channels and converting it to RGB to obtain the final colour image.","metadata":{}},{"cell_type":"code","source":"def to_rgb(grayscale_input, ab_output, save_path=None, save_name=None):\n    color_image = torch.cat((grayscale_input, ab_output), 0).numpy() # combine channels\n    color_image = color_image.transpose((1, 2, 0))  # rescale for matplotlib\n    color_image[:, :, 0:1] = color_image[:, :, 0:1]\n    color_image[:, :, 1:3] = (color_image[:, :, 1:3]) * 128\n    color_image = lab2rgb(color_image.astype(np.float64))\n    grayscale_input = grayscale_input.squeeze().numpy()\n    return color_image","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Given a black and white image with 3 channels(R==G==B), converting it to Lab and giving the L channel as input to predict the ab channels and finally, obtaining the predicted coloured version of the image.","metadata":{}},{"cell_type":"code","source":"def prediction(img):\n    a = rgb2lab(img.permute(1, 2, 0))\n    a = torch.tensor(a[:,:,0]).type(torch.FloatTensor)\n    a = a.unsqueeze(0)\n    a = a.unsqueeze(0)\n    xb = to_device(a, device)\n    ab_img = model(xb)\n    xb = xb.squeeze(0)\n    ab_img = ab_img.squeeze(0)\n    return to_rgb(xb.detach().cpu(), ab_img.detach().cpu())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Testing the model on different black and white images.","metadata":{}},{"cell_type":"code","source":"import glob\nfrom PIL import Image\n\nimages = glob.glob(\"../input/test-images/*jpg\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for img in range(len(images)):\n    image = Image.open(images[img])\n    trans1 = T.Resize((256, 256))\n    trans2 = T.ToTensor()\n    images[img] = trans2(trans1(image))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img = images[1]\nf, arr = plt.subplots(1, 2, sharey=True)\narr[0].imshow(img.permute(1, 2, 0))\narr[1].imshow(prediction(img))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}