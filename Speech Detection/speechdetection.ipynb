{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        pass\n#         print(os.path.join(dirname, filename))\n\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"0ed30ad1-117e-46b1-9349-b2b9e8dceb7e","_cell_guid":"769732b9-a47c-4e7f-9fa7-4bf9eabd6ce7","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-07-02T10:02:26.275341Z","iopub.execute_input":"2022-07-02T10:02:26.275942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd       \nimport os \nimport math \nimport numpy as np\nimport matplotlib.pyplot as plt  \nimport IPython.display as ipd  # To play sound in the notebook\nimport librosa\nimport librosa.display\nfrom tqdm import *\nfrom keras.callbacks import (EarlyStopping, LearningRateScheduler,\n                             ModelCheckpoint, TensorBoard, ReduceLROnPlateau)\n!apt install -y ffmpeg\n# os.chdir(\"/kaggle/input/freesound-audio-tagging/audio_train\")\n#os.getcwd()\nos.chdir(\"/kaggle/input/speech-accent-archive/recordings\")\nimport warnings\nwarnings.filterwarnings('ignore')\n\nOUTPUT_DIR = '/kaggle/working/'\n","metadata":{"_uuid":"d70bcc2c-8b33-4576-a02b-dc6c772ef3ce","_cell_guid":"ecb2eaa9-d15e-420f-9ba8-860766a49f61","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Play female from Kentucky\nfname_f = 'recordings/' + 'english385.mp3'   \nipd.Audio(fname_f)","metadata":{"_uuid":"3cb142ac-2f0a-4f07-b409-a88d1f6e597d","_cell_guid":"9bd552a5-6eab-4d33-9e85-a53edf182543","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"filename = '/kaggle/input/common-voice/cv-other-test/cv-other-test/sample-001204.mp3'\nipd.Audio(filename)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Play male from Kentucky\nfname_m = 'recordings/' + 'english381.mp3'\nipd.Audio(fname_m)","metadata":{"_uuid":"404f5d76-076f-48c9-8015-5814eee7a7ee","_cell_guid":"3d4c6632-4549-453c-9c48-02a14f025f20","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# MFCC for female \nSAMPLE_RATE = 22050\nfname_f = 'recordings/' + 'english385.mp3'\n\ny, sr = librosa.load(fname_f, sr=SAMPLE_RATE, duration = 20)# Chop audio at 10 secs...\nmfcc = librosa.feature.mfcc(y=y, sr=SAMPLE_RATE, n_mfcc = 13)# 10 MFCC components\n\nplt.figure(figsize=(15, 7))\nplt.subplot(3,1,1)\nlibrosa.display.specshow(mfcc)\nplt.ylabel('MFCC')\nplt.colorbar()","metadata":{"_uuid":"a8ba1655-6476-4e67-a73c-c815c2e3aa29","_cell_guid":"54878a6e-1c63-4ce0-8032-43562f39277a","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# MFCC for male  \nSAMPLE_RATE = 22050\nfname_m = 'recordings/' + 'english381.mp3'  \ny, sr = librosa.load(fname_m, sr=SAMPLE_RATE, duration = 20)\nmfcc = librosa.feature.mfcc(y=y, sr=SAMPLE_RATE, n_mfcc = 13)\n\nplt.figure(figsize=(15, 7))\nplt.subplot(3,1,1)\nlibrosa.display.specshow(mfcc, x_axis='time')\nplt.ylabel('MFCC')\nplt.colorbar()","metadata":{"_uuid":"d489acd2-afb6-4cad-98a6-06f291177c43","_cell_guid":"b7ecc8c4-17ff-4ae1-9fd0-b23aad53ca30","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/speech-accent-archive/speakers_all.csv')","metadata":{"_uuid":"9d9aa687-1608-4186-b3d5-c9f91375017f","_cell_guid":"f64fe970-cf6b-42ad-b4e7-97e1dbe3ce3a","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"_uuid":"9c703657-8bed-41cb-b045-fe0d60a10de3","_cell_guid":"0c324521-947d-4aea-8cba-f4125a2e03c7","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.tail()","metadata":{"_uuid":"8c74ad46-3ddd-4eaa-8b32-1a5126f2a93f","_cell_guid":"13385ff8-8a99-4048-8c3b-632706dc1d10","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data[data['native_language']=='english']","metadata":{"_uuid":"50cac7a0-6c91-4c43-8b50-353c03082a0c","_cell_guid":"15ab8f58-f540-4297-9420-a370dd42c663","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data[data['file_missing?']=='True'].count()","metadata":{"_uuid":"f0cee248-d551-4c8d-9742-b3595035ed79","_cell_guid":"1c90d159-b41f-4b0b-bfdd-3405ffd35e0d","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's found gender class distribution:","metadata":{"_uuid":"d9d4009c-0988-447c-978d-ea5c43f01967","_cell_guid":"d73d534d-694d-425f-aaf7-98115aa33076","trusted":true}},{"cell_type":"code","source":"data['sex'].value_counts()","metadata":{"_uuid":"52b07f8c-69d6-4972-94f5-00bcd1bb5dd1","_cell_guid":"18e79d2e-4d76-4ce3-a5df-ea9cef8bb194","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"res=data['native_language'].value_counts()","metadata":{"_uuid":"7e19f740-3221-4eb1-9d1a-924bce8ca92f","_cell_guid":"d36bb821-6335-46a1-a7ac-ff13cd5a7051","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"res1 =data[data['native_language']!='english'].native_language.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"res1[res>40].sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dg=data.groupby(\"native_language\").filter(lambda x: len(x) >40)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Unseen data for testing later\ntest_data = data.groupby(\"native_language\").filter(lambda x: len(x) <40)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dg.sex.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dg['native_language'].value_counts()","metadata":{"_uuid":"9595bc93-7f09-48ed-a4a5-7b27516dd9f9","_cell_guid":"a88feb89-21c8-4614-9b35-638e298a6bf6","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dg.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dg['filename'] = dg['filename'].apply(lambda x: x+'.mp3')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dg.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dg.drop(columns=['Unnamed: 9', 'Unnamed: 10', 'Unnamed: 11'],inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dg.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dg['accent'] = dg['native_language'].apply(lambda x: 'native' if x=='english' else 'non-native')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dg['accent'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def feature_extractor(files, feature_type='mfcc'):\n    features = []\n    SAMPLE_RATE = 22050\n    dire = 'recordings/'\n    if feature_type == 'mfcc':\n        for file in files:\n            f_name = str(dire+file)\n            y, sr = librosa.load(f_name, sr=SAMPLE_RATE, duration = 10)\n            mfcc = librosa.feature.mfcc(y=y, sr=SAMPLE_RATE, n_mfcc = 10)\n            features.append(mfcc)\n    return features","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# features = feature_extractor(dg['filename'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# features[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Config(object):\n    def __init__(self,sampling_rate=16000, audio_duration=2, n_classes=10, learning_rate=0.0001, max_epochs=20, n_mfcc=40):\n        self.sampling_rate = sampling_rate\n        self.audio_duration = audio_duration\n        self.n_classes = n_classes\n        self.n_mfcc = n_mfcc\n        self.learning_rate = learning_rate\n        self.max_epochs = max_epochs\n        self.audio_length = self.sampling_rate * self.audio_duration\n        self.dim = (self.n_mfcc, 1 + int(np.floor(self.audio_length/512)), 1)\n\ndef prepare_data(fnames, config, data_dir):\n    X = np.empty(shape=(len(fnames), config.dim[0], config.dim[1], 1))\n    input_length = config.audio_length\n    for i, fname in tqdm_notebook(enumerate(fnames), total=len(fnames)):\n        file_path = os.path.join(data_dir, fname)\n        data, _ = librosa.core.load(file_path, sr=config.sampling_rate, res_type=\"kaiser_fast\")\n\n        # Random offset / Padding\n        if len(data) > input_length:\n            max_offset = len(data) - input_length\n            offset = np.random.randint(max_offset)\n            data = data[offset:(input_length+offset)]\n        else:\n            if input_length > len(data):\n                max_offset = input_length - len(data)\n                offset = np.random.randint(max_offset)\n            else:\n                offset = 0\n            data = np.pad(data, (offset, input_length - len(data) - offset), \"constant\")\n\n        data = librosa.feature.mfcc(data, sr=config.sampling_rate, n_mfcc=config.n_mfcc)\n        data = np.expand_dims(data, axis=-1)\n        X[i,] = data\n    return X\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_fnames = np.array(dg['filename'])\ndata_path = 'recordings/'\n\nconfig = Config(sampling_rate=22050, audio_duration=20, learning_rate=0.0001, n_mfcc=13, n_classes=2)\nfeature_file_path = OUTPUT_DIR+'mfcc_features.npy'\nfeature_file_path = '/kaggle/input/speechdetection/mfcc_features.npy'\nif os.path.exists(feature_file_path):\n    X = np.load(feature_file_path)\nelse:\n    X = prepare_data(X_fnames, config, data_path)\n    np.save(feature_file_path, X)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# with open(OUTPUT_DIR+'mfcc_features.npy', 'wb+') as f:\n#     np.save(f, X)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.utils import to_categorical\nfrom sklearn.preprocessing import LabelEncoder\n\ny_gender = dg['sex']\ny_accent = dg['accent']\n\nencoder = LabelEncoder()\nencoder.fit(y_gender)\ny_gender = encoder.transform(y_gender)\n\nencoder1 = LabelEncoder()\nencoder1.fit(y_accent)\ny_accent = encoder1.transform(y_accent)\ny_gender = to_categorical(np.array(y_gender))\ny_accent = to_categorical(np.array(y_accent))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Normalization\n\nNormalization is a crucial preprocessing step. The simplest method is rescaling the range of features to scale the range in [0, 1].","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_gender_train, y_gender_test, y_accent_train, y_accent_test = train_test_split(X, y_gender, y_accent, test_size=0.2, random_state=10)\nprint ('Train set:', X_train.shape,  y_gender_train.shape)\nprint ('Test set:', X_test.shape,  y_gender_test.shape)\n\nX_train, X_val, y_gender_train, y_gender_val, y_accent_train, y_accent_val = train_test_split(X_train, y_gender_train, y_accent_train, test_size=0.15, random_state=10)\n\nmean = np.mean(X_train, axis=0)\nstd = np.std(X_train, axis=0)\n\nX_train = (X_train - mean)/std\nX_val = (X_val - mean)/std\nX_test = (X_test - mean)/std","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print ('Label accent set:', y_accent_train.shape,  y_accent_test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Simple Multilayer Feedforward Neural Network","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Dense,Dropout,Activation, Flatten, Input, Conv2D, BatchNormalization, MaxPooling2D, LSTM, Concatenate\nfrom tensorflow.keras.optimizers import Adam\nfrom sklearn import metrics\nimport tensorflow as tf\nfrom keras.regularizers import l2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_layer = Input(shape=(13, 862), name='Input')\n\ndense1=Dense(128, activation='relu',name='Hidden_1')(input_layer)\ndropout1 = Dropout(0.5)(dense1)\n\ndense2=Dense(256, activation='relu',name='Hidden_2')(dropout1)\ndropout2 = Dropout(0.5)(dense2)\n\ndense3=Dense(128, activation='relu',name='Hidden_3')(dropout2)\ndropout3 = Dropout(0.5)(dense3)\nm = tf.keras.layers.Flatten()(dropout3)\n\nout_accent = Dense(2, activation='sigmoid',name='output_accent')(m)\nout_gender = Dense(2, activation='sigmoid',name='output_gender')(m)\n\n\nmodel_ffn = Model(inputs = input_layer ,outputs=[out_accent, out_gender])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_ffn.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_ffn.compile(optimizer='Adam',loss={'output_accent':'binary_crossentropy','output_gender': 'binary_crossentropy'}, metrics ={'output_accent': 'accuracy', 'output_gender': 'accuracy'})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Training my model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom datetime import datetime \n\nnum_epochs = 70\nnum_batch_size = 32\n\ncheckpointer = ModelCheckpoint(filepath=OUTPUT_DIR + 'saved_models/audio_classification_ffn.hdf5', mode='min',\n                               verbose=1, save_best_only=True,patience=10, monitor='val_loss')\nearly_stopping = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=10)\n\nstart = datetime.now()\n\nhistory = model_ffn.fit(X_train, {\"output_accent\": y_accent_train, \"output_gender\": y_gender_train},validation_data=(X_val,{\"output_accent\": y_accent_val, \"output_gender\": y_gender_val}),\n                        batch_size=num_batch_size, epochs=num_epochs, callbacks=[checkpointer, early_stopping], verbose=1)\n\n\nduration = datetime.now() - start\nprint(\"Training completed in time: \", duration)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Evaluation","metadata":{}},{"cell_type":"code","source":"results = model_ffn.evaluate(X_test, {\"output_accent\": y_accent_test, \"output_gender\": y_gender_test})\nprint(results)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot training and validation accuracy values\nplt.plot(history.history['output_accent_accuracy'])\nplt.plot(history.history['val_output_accent_accuracy'])\nplt.plot(history.history['output_gender_accuracy'])\nplt.plot(history.history['val_output_gender_accuracy'])\nplt.title('Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train(Accent)','Validation(Accent)', 'Train(Gender)','Validation(Gender)'], loc = 'upper left')\nplt.show()\n\n# Plot training and validation accuracy values\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train','Validation'], loc = 'upper left')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accent_prediction, gender_prediction = model_ffn.predict(X_test)\nprediction_accent_rounded = [np.argmax(i) for i in accent_prediction]\n# prediction_ANN_rounded[0]\ny_test_index = [np.argmax(i) for i in y_accent_test]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n\nprint(classification_report(y_test_index, prediction_accent_rounded))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\n\n#Confusion Matrix - verify accuracy of each class\nimport seaborn as sns\ncm = tf.math.confusion_matrix(labels = y_test_index, predictions = prediction_accent_rounded)\nplt.figure(figsize = (12,7))\nsns.heatmap(cm,annot=True, fmt='d')\nplt.xlabel('Prediction')\nplt.ylabel('True_value')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prediction_gender_rounded = [np.argmax(i) for i in gender_prediction]\n# prediction_ANN_rounded[0]\ny_test_gender_index = [np.argmax(i) for i in y_gender_test]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cm = tf.math.confusion_matrix(labels = y_test_gender_index, predictions = prediction_gender_rounded)\nplt.figure(figsize = (12,7))\nsns.heatmap(cm,annot=True, fmt='d')\nplt.xlabel('Prediction')\nplt.ylabel('True_value')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n\nprint(classification_report(y_test_gender_index, prediction_gender_rounded))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CNN","metadata":{}},{"cell_type":"markdown","source":"### CNN- Accent classification","metadata":{}},{"cell_type":"code","source":"def create_cnn_model(input_shape=None):\n    # Create CNN model\n    input_layer = Input(shape=(13, 862, 1), name='Input')\n    # conv 1\n    conv = Conv2D(32, 3, padding='same', activation='relu')(input_layer)\n    batchnorm = BatchNormalization()(conv)\n    maxpool = MaxPooling2D(pool_size=(2, 2))(batchnorm)\n    batchnorm1 = BatchNormalization()(maxpool)\n    # conv 2\n    conv1 = Conv2D(32, 3, padding='same', activation='relu')(batchnorm1)\n    batchnorm2 = BatchNormalization()(conv1)\n    maxpool1 = MaxPooling2D(pool_size=(2, 2))(batchnorm2)\n    batchnorm3 = BatchNormalization()(maxpool1)\n    # conv 3\n    conv2 = Conv2D(128, 3, padding='same', activation='relu', name='conv2')(batchnorm3)\n    batchnorm4 = BatchNormalization()(conv2)\n    maxpool2 = MaxPooling2D(pool_size=(2, 2))(batchnorm4)\n    batchnorm5 = BatchNormalization()(maxpool2)\n    # flatten\n    flatten = Flatten()(batchnorm5)\n    dense = Dense(64, activation='relu',name='Hidden_1')(flatten)\n    dropout = Dropout(0.5)(dense)\n    # output\n    out = Dense(2, activation='sigmoid',name='output')(dropout)\n    model = Model(inputs = input_layer ,outputs=out)\n    return model\n","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_cnn_accent = create_cnn_model()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_cnn_accent.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tf.keras.utils.plot_model(model_cnn_accent, to_file='model.png', show_shapes=True, show_layer_names=False)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_cnn_accent.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer='adam')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Trianing my model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom datetime import datetime \n\nnum_epochs = 35\nnum_batch_size = 32\n\ncheckpointer = ModelCheckpoint(filepath=OUTPUT_DIR +'saved_models/accent_classification_cnn.h5', \n                               verbose=1, save_best_only=True,patience=10, mode='max', monitor='val_accuracy')\nearly_stopping = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=5)\n\nstart = datetime.now()\n\nhistory = model_cnn_accent.fit(X_train, y_accent_train, batch_size=num_batch_size, epochs=num_epochs, validation_data=(X_val, y_accent_val), callbacks=[checkpointer,early_stopping], verbose=1)\n\n\nduration = datetime.now() - start\nprint(\"Training completed in time: \", duration)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results_accent = model_cnn_accent.evaluate(X_test, y_accent_test)\nprint(results_accent)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot training and validation accuracy values\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train','Validation'], loc = 'upper left')\nplt.show()\n\n# Plot training and validation accuracy values\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train','Validation'], loc = 'upper left')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### CNN - Gender classification","metadata":{}},{"cell_type":"code","source":"model_cnn_gender = create_cnn_model()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_cnn_gender.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer='adam')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Trianing my model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom datetime import datetime \n\nnum_epochs = 35\nnum_batch_size = 32\n\ncheckpointer = ModelCheckpoint(filepath=OUTPUT_DIR +'saved_models/gender_classification_cnn.h5', \n                               verbose=1, save_best_only=True,patience=5, mode='max', monitor='val_accuracy')\nearly_stopping = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=5)\n\nstart = datetime.now()\n\nhistory_gender = model_cnn_gender.fit(X_train, y_gender_train, batch_size=num_batch_size, epochs=num_epochs, validation_data=(X_val, y_gender_val), callbacks=[checkpointer, early_stopping], verbose=1)\n\n\nduration = datetime.now() - start\nprint(\"Training completed in time: \", duration)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results_gender = model_cnn_gender.evaluate(X_test, y_gender_test)\nprint(results_gender)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot training and validation accuracy values\nplt.plot(history_gender.history['accuracy'])\nplt.plot(history_gender.history['val_accuracy'])\nplt.title('Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train','Validation'], loc = 'upper left')\nplt.show()\n\n# Plot training and validation accuracy values\nplt.plot(history_gender.history['loss'])\nplt.plot(history_gender.history['val_loss'])\nplt.title('Model Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train','Validation'], loc = 'upper left')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Multi-task Learning using ConvNets: Accent and Gender","metadata":{}},{"cell_type":"code","source":"def create_multitask_cnn():\n    # Create CNN model\n    input_layer = Input(shape=(13, 862,1), name='Input')\n    # conv 1\n    conv = Conv2D(32, 3, padding='same', activation='relu')(input_layer)\n    batchnorm = BatchNormalization()(conv)\n    maxpool = MaxPooling2D(pool_size=(2, 2))(batchnorm)\n    batchnorm1 = BatchNormalization()(maxpool)\n    # conv 2\n    conv1 = Conv2D(32, 3, padding='same', activation='relu')(batchnorm1)\n    batchnorm2 = BatchNormalization()(conv1)\n    maxpool1 = MaxPooling2D(pool_size=(2, 2))(batchnorm1)\n    batchnorm3 = BatchNormalization()(maxpool1)\n    # conv 3\n    conv2 = Conv2D(128, 3, padding='same', activation='relu')(batchnorm3)\n    batchnorm4 = BatchNormalization()(conv2)\n    maxpool2 = MaxPooling2D(pool_size=(2, 2))(batchnorm4)\n    batchnorm5 = BatchNormalization()(maxpool2)\n    # flatten\n    flatten = Flatten()(batchnorm5)\n    dense = Dense(64, activation='relu',name='Hidden_1')(flatten)\n    dropout = Dropout(0.5)(dense)\n    # output\n    out_accent = Dense(2, activation='sigmoid',name='output_accent')(dropout)\n    out_gender = Dense(2, activation='sigmoid',name='output_gender')(dropout)\n\n    model_cnn = Model(inputs = input_layer ,outputs=[out_accent, out_gender])\n    return model_cnn","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"multitask_cnn = create_multitask_cnn()\nmultitask_cnn.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"multitask_cnn.compile(optimizer='Adam',loss={'output_accent':'binary_crossentropy','output_gender': 'binary_crossentropy'}, metrics ={'output_accent': 'accuracy', 'output_gender': 'accuracy'})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Training my model\n\nnum_epochs = 35\nnum_batch_size = 32\n\ncheckpointer = ModelCheckpoint(filepath=OUTPUT_DIR + 'saved_models/multitask_classification_cnn.h5', \n                               verbose=1, save_best_only=True,patience=10, monitor='val_loss', mode='min')\nearly_stopping = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=10)\n\nstart = datetime.now()\n\nhistory_multitask_cnn = multitask_cnn.fit(X_train, {\"output_accent\": y_accent_train, \"output_gender\": y_gender_train},validation_data=(X_val,{\"output_accent\": y_accent_val, \"output_gender\": y_gender_val}),\n                        batch_size=num_batch_size, epochs=num_epochs, callbacks=[checkpointer], verbose=1)\n\n\nduration = datetime.now() - start\nprint(\"Training completed in time: \", duration)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results_multitask_cnn = multitask_cnn.evaluate(X_test, {\"output_accent\": y_accent_test, \"output_gender\": y_gender_test})\nprint(results_multitask_cnn)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accent_prediction, gender_prediction = multitask_cnn.predict(X_test)\nprediction_accent_rounded = [np.argmax(i) for i in accent_prediction]\n# prediction_ANN_rounded[0]\ny_test_index = [np.argmax(i) for i in y_accent_test]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\n\n#Confusion Matrix - verify accuracy of each class\nimport seaborn as sns\ncm = tf.math.confusion_matrix(labels = y_test_index, predictions = prediction_accent_rounded)\nplt.figure(figsize = (12,7))\nsns.heatmap(cm,annot=True, fmt='d')\nplt.xlabel('Prediction')\nplt.ylabel('True_value')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prediction_gender_rounded = [np.argmax(i) for i in gender_prediction]\n# prediction_ANN_rounded[0]\ny_test_gender_index = [np.argmax(i) for i in y_gender_test]\ncm = tf.math.confusion_matrix(labels = y_test_gender_index, predictions = prediction_gender_rounded)\nplt.figure(figsize = (12,7))\nsns.heatmap(cm,annot=True, fmt='d')\nplt.xlabel('Prediction')\nplt.ylabel('True_value')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n\nprint(classification_report(y_test_gender_index, prediction_gender_rounded))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_test_index, prediction_accent_rounded))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot training and validation accuracy values\nplt.plot(history_multitask_cnn.history['output_accent_accuracy'])\nplt.plot(history_multitask_cnn.history['val_output_accent_accuracy'])\nplt.plot(history_multitask_cnn.history['output_gender_accuracy'])\nplt.plot(history_multitask_cnn.history['val_output_gender_accuracy'])\nplt.title('Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train(Accent)','Validation(Accent)', 'Train(Gender)','Validation(Gender)'], loc = 'upper left')\nplt.show()\n\n# Plot training and validation accuracy values\nplt.plot(history_multitask_cnn.history['loss'])\nplt.plot(history_multitask_cnn.history['val_loss'])\nplt.title('Model Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train','Validation'], loc = 'upper left')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LSTM","metadata":{}},{"cell_type":"markdown","source":"### LSTM - Accent classification","metadata":{}},{"cell_type":"code","source":"def create_LSTM():\n    # Create CNN model\n    input_layer = Input(shape=(13, 862), name='Input')\n    \n    lstm_1 = LSTM(64, return_sequences=True)(input_layer)\n    lstm_2 = LSTM(64)(lstm_1)\n    dense = Dense(64, activation='relu',name='Hidden_1')(lstm_2)\n    \n#     lstm_3 = LSTM(64, stateful=False)(lstm_2)\n    dropout = Dropout(0.5)(dense)\n    # output\n    out_accent = Dense(2, activation='sigmoid',name='output_accent')(dropout)\n#     out_gender = Dense(2, activation='sigmoid',name='output_gender')(dropout)\n\n    model = Model(inputs = input_layer ,outputs=out_accent)\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accent_lstm = create_LSTM()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accent_lstm.compile(optimizer='Adam',loss='binary_crossentropy', metrics =['accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Trianing my model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom datetime import datetime \n\nnum_epochs = 35\nnum_batch_size = 32\n\ncheckpointer = ModelCheckpoint(filepath=OUTPUT_DIR +'saved_models/accent_classification.h5', \n                               verbose=1, save_best_only=True,patience=10, mode='max', monitor='val_accuracy')\nearly_stopping = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=7)\n\nstart = datetime.now()\n\nhistory_accent_lstm = accent_lstm.fit(X_train, y_accent_train, batch_size=num_batch_size, epochs=num_epochs, validation_data=(X_val, y_accent_val), callbacks=[checkpointer, early_stopping], verbose=1)\n\n\nduration = datetime.now() - start\nprint(\"Training completed in time: \", duration)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results_accent_lstm = accent_lstm.evaluate(X_test, y_accent_test)\nprint(results_accent_lstm)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### LSTM multitask","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_LSTM_multi():\n    # Create CNN model\n    input_layer = Input(shape=(13, 862), name='Input')\n    \n#     lstm_1 = LSTM(64, return_sequences=True)(input_layer)\n    lstm_2 = LSTM(100)(input_layer)\n    \n#     lstm_3 = LSTM(64, stateful=False)(lstm_2)\n    dropout = Dropout(0.25)(lstm_2)\n    # output\n    out_accent = Dense(2, activation='sigmoid',name='output_accent')(dropout)\n    out_gender = Dense(2, activation='sigmoid',name='output_gender')(dropout)\n\n    model_cnn = Model(inputs = input_layer ,outputs=[out_accent, out_gender])\n    return model_cnn","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"multitask_lstm = create_LSTM_multi()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"multitask_lstm.compile(optimizer='Adam',loss={'output_accent':'binary_crossentropy','output_gender': 'binary_crossentropy'}, metrics ={'output_accent': 'accuracy', 'output_gender': 'accuracy'})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Training my model\n\nnum_epochs = 35\nnum_batch_size = 32\n\ncheckpointer = ModelCheckpoint(filepath=OUTPUT_DIR + 'saved_models/multitask_classification_lstm.h5', \n                               verbose=1, save_best_only=True,patience=10, monitor='val_loss', mode='min')\nearly_stopping = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=10)\n\nstart = datetime.now()\n\nhistory_multitask_lstm = multitask_lstm.fit(X_train, {\"output_accent\": y_accent_train, \"output_gender\": y_gender_train},validation_data=(X_val,{\"output_accent\": y_accent_val, \"output_gender\": y_gender_val}),\n                        batch_size=num_batch_size, epochs=num_epochs, callbacks=[checkpointer], verbose=1)\n\n\nduration = datetime.now() - start\nprint(\"Training completed in time: \", duration)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results_multitask_lstm = multitask_lstm.evaluate(X_test, {\"output_accent\": y_accent_test, \"output_gender\": y_gender_test})\nprint(results_multitask_lstm)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CRNN","metadata":{}},{"cell_type":"markdown","source":"### CRNN - Accent classificaiton","metadata":{}},{"cell_type":"code","source":"from keras.layers.core import Dense, Permute, Reshape\nfrom keras.layers.wrappers import Bidirectional\ndef create_CRNN_model(input_shape, config, is_training=True):\n    \n    model = Sequential()\n    \n    model.add(Conv2D(32, 3, activation=\"relu\",padding='same', input_shape=input_shape))\n    model.add(BatchNormalization())\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n\n    model.add(Conv2D(32, 3, activation=\"relu\",padding='same'))\n    model.add(BatchNormalization())\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n\n    model.add(Conv2D(128, 3, activation=\"relu\",padding='same'))\n    model.add(BatchNormalization())\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n\n#     model.add(Conv2D(256, 3, activation=\"relu\",padding='same'))\n#     model.add(BatchNormalization())\n#     model.add(MaxPooling2D(pool_size=(2, 2)))\n\n#     model.add(Conv2D(512, 3, 3, W_regularizer=l2(weight_decay), activation=\"relu\"))\n#     model.add(BatchNormalization())\n    # model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n\n#     model.add(Convolution2D(512, 3, 3, W_regularizer=l2(weight_decay), activation=\"relu\"))\n#     model.add(BatchNormalization())\n#     model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n\n#     model.add(Conv2D(512, 3, 3, W_regularizer=l2(weight_decay), activation=\"relu\"))\n#     model.add(BatchNormalization())\n#     model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n    \n\n#     input_layer = Input(shape=(13, 862,1), name='Input')\n#     # conv 1\n#     conv = Conv2D(32, 3, padding='same', activation='relu')(input_layer)\n#     batchnorm = BatchNormalization()(conv)\n#     maxpool = MaxPooling2D(pool_size=(2, 2))(batchnorm)\n#     batchnorm1 = BatchNormalization()(maxpool)\n#     # conv 2\n#     conv1 = Conv2D(32, 3, padding='same', activation='relu')(batchnorm1)\n#     batchnorm2 = BatchNormalization()(conv1)\n#     maxpool1 = MaxPooling2D(pool_size=(2, 2))(batchnorm1)\n#     batchnorm3 = BatchNormalization()(maxpool1)\n#     # conv 3\n#     conv2 = Conv2D(128, 3, padding='same', activation='relu')(batchnorm3)\n#     batchnorm4 = BatchNormalization()(conv2)\n#     maxpool2 = MaxPooling2D(pool_size=(2, 2))(batchnorm4)\n#     batchnorm5 = BatchNormalization()(maxpool2)\n\n    # (bs, y, x, c) --> (bs, x, y, c)\n    model.add(Permute((2, 1, 3)))\n\n    # (bs, x, y, c) --> (bs, x, y * c)\n    bs, x, y, c = model.layers[-1].output_shape\n    model.add(Reshape((x, y*c)))\n\n    model.add(Bidirectional(LSTM(256, return_sequences=False), merge_mode=\"concat\"))\n    model.add(Dense(2, activation=\"sigmoid\"))\n\n\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"crnn_model = create_CRNN_model((13,862,1), config)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"crnn_model.compile(optimizer='Adam',loss='binary_crossentropy', metrics =['accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Trianing my model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom datetime import datetime \n\nnum_epochs = 35\nnum_batch_size = 32\n\ncheckpointer = ModelCheckpoint(filepath=OUTPUT_DIR +'saved_models/accent_crnn_classification.h5', \n                               verbose=1, save_best_only=True,patience=10, mode='max', monitor='val_accuracy')\nearly_stopping = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=5)\n\nstart = datetime.now()\n\nhistory_accent_lstm = crnn_model.fit(X_train, y_accent_train, batch_size=num_batch_size, epochs=num_epochs, validation_data=(X_val, y_accent_val), callbacks=[checkpointer, early_stopping], verbose=1)\n\n\nduration = datetime.now() - start\nprint(\"Training completed in time: \", duration)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.models import load_model\ncrnn_accent_model = load_model('/kaggle/working/saved_models/accent_crnn_classification.h5')\nresults_accent_crnn = crnn_accent_model.evaluate(X_test, y_accent_test)\nprint(results_accent_crnn)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### CRNN - Gender Classification ","metadata":{}},{"cell_type":"code","source":"crnn_model_gender = create_CRNN_model((13,862,1), config)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"crnn_model_gender.compile(optimizer='Adam',loss='binary_crossentropy', metrics =['accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Trianing my model\n\ncheckpointer = ModelCheckpoint(filepath=OUTPUT_DIR +'saved_models/gender_crnn_classification.h5', \n                               verbose=1, save_best_only=True,patience=10, mode='max', monitor='val_accuracy')\nearly_stopping = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=5)\n\nstart = datetime.now()\n\nhistory_gender_crnn = crnn_model_gender.fit(X_train, y_gender_train, batch_size=num_batch_size, epochs=num_epochs, validation_data=(X_val, y_gender_val), callbacks=[checkpointer, early_stopping], verbose=1)\n\n\nduration = datetime.now() - start\nprint(\"Training completed in time: \", duration)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.models import load_model\ncrnn_gender_model = load_model('/kaggle/working/saved_models/gender_crnn_classification.h5')\nresults_agender_crnn = crnn_gender_model.evaluate(X_test, y_gender_test)\nprint(results_agender_crnn)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Multitask Learning- CRNN","metadata":{}},{"cell_type":"code","source":"def create_CRNN_multitask_model():\n    \n#     model = Sequential()\n    \n#     model.add(Conv2D(64, 3, activation=\"relu\",padding='same', input_shape=input_shape))\n#     model.add(BatchNormalization())\n#     model.add(MaxPooling2D(pool_size=(2, 2)))\n\n#     model.add(Conv2D(128, 3, activation=\"relu\",padding='same'))\n#     model.add(BatchNormalization())\n#     model.add(MaxPooling2D(pool_size=(2, 2)))\n\n#     model.add(Conv2D(128, 3, activation=\"relu\",padding='same'))\n#     model.add(BatchNormalization())\n#     model.add(MaxPooling2D(pool_size=(2, 2)))\n\n#     model.add(Conv2D(256, 3, activation=\"relu\",padding='same'))\n#     model.add(BatchNormalization())\n#     model.add(MaxPooling2D(pool_size=(2, 2)))\n\n#     model.add(Conv2D(512, 3, 3, W_regularizer=l2(weight_decay), activation=\"relu\"))\n#     model.add(BatchNormalization())\n    # model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n\n#     model.add(Convolution2D(512, 3, 3, W_regularizer=l2(weight_decay), activation=\"relu\"))\n#     model.add(BatchNormalization())\n#     model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n\n#     model.add(Conv2D(512, 3, 3, W_regularizer=l2(weight_decay), activation=\"relu\"))\n#     model.add(BatchNormalization())\n#     model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n    \n\n    input_layer = Input(shape=(13, 862,1), name='Input')\n    # conv 1\n    conv = Conv2D(32, 3, padding='same', activation='relu')(input_layer)\n    batchnorm = BatchNormalization()(conv)\n    maxpool = MaxPooling2D(pool_size=(2, 2))(batchnorm)\n    # conv 2\n    conv1 = Conv2D(32, 3, padding='same', activation='relu')(maxpool)\n    batchnorm2 = BatchNormalization()(conv1)\n    maxpool1 = MaxPooling2D(pool_size=(2, 2))(batchnorm2)\n\n    # conv 3\n    conv2 = Conv2D(128, 3, padding='same', activation='relu')(maxpool1)\n    batchnorm4 = BatchNormalization()(conv2)\n    maxpool2 = MaxPooling2D(pool_size=(2, 2))(batchnorm4)\n    \n    permute = Permute((2, 1, 3))(maxpool2)\n    reshaped = Reshape((107, 128))(permute)\n\n    lstm = Bidirectional(LSTM(256, return_sequences=False))(reshaped)\n    \n    out_accent = Dense(2, activation='sigmoid',name='output_accent')(lstm)\n    out_gender = Dense(2, activation='sigmoid',name='output_gender')(lstm)\n\n    # (bs, y, x, c) --> (bs, x, y, c)\n#     model.add(Permute((2, 1, 3)))\n\n#     # (bs, x, y, c) --> (bs, x, y * c)\n#     bs, x, y, c = model.layers[-1].output_shape\n#     model.add(Reshape((x, y*c)))\n\n#     model.add(Bidirectional(LSTM(256, return_sequences=False), merge_mode=\"concat\"))\n#     model.add([Dense(2, activation=\"sigmoid\", name='output_accent'),Dense(2, activation=\"sigmoid\", name='output_gender')])\n    model = Model(inputs = input_layer ,outputs=[out_accent, out_gender])\n\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"multitask_crnn = create_CRNN_multitask_model()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"multitask_crnn.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"multitask_crnn.compile(optimizer='Adam',loss={'output_accent':'binary_crossentropy','output_gender': 'binary_crossentropy'}, metrics ={'output_accent': 'accuracy', 'output_gender': 'accuracy'})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Training my model\n\nnum_epochs = 35\nnum_batch_size = 32\n\ncheckpointer = ModelCheckpoint(filepath=OUTPUT_DIR + 'saved_models/multitask_classification_crnn.h5', \n                               verbose=1, save_best_only=True,patience=10, monitor='val_loss', mode='min')\nearly_stopping = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=5)\n\nstart = datetime.now()\n\nhistory_multitask_crnn = multitask_crnn.fit(X_train, {\"output_accent\": y_accent_train, \"output_gender\": y_gender_train},validation_data=(X_val,{\"output_accent\": y_accent_val, \"output_gender\": y_gender_val}),\n                        batch_size=num_batch_size, epochs=num_epochs, callbacks=[checkpointer, early_stopping], verbose=1)\n\n\nduration = datetime.now() - start\nprint(\"Training completed in time: \", duration)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.models import load_model\ncrnn_multi_model = load_model('/kaggle/working/saved_models/multitask_classification_crnn.h5')\nresults_multitask_crnn = crnn_multi_model.evaluate(X_test, {\"output_accent\": y_accent_test, \"output_gender\": y_gender_test})\nprint(results_multitask_crnn)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accent_prediction, gender_prediction = crnn_multi_model.predict(X_test)\n\nprediction_accent_rounded = [np.argmax(i) for i in accent_prediction]\n\n# prediction_ANN_rounded[0]\n\ny_test_index = [np.argmax(i) for i in y_accent_test]\n\n\nimport tensorflow as tf\n\n\n#Confusion Matrix - verify accuracy of each class\n\nimport seaborn as sns\n\ncm = tf.math.confusion_matrix(labels = y_test_index, predictions = prediction_accent_rounded)\n\nplt.figure(figsize = (12,7))\n\nsns.heatmap(cm,annot=True, fmt='d')\n\nplt.xlabel('Prediction')\n\nplt.ylabel('True_value')\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prediction_gender_rounded = [np.argmax(i) for i in gender_prediction]\n\n# prediction_ANN_rounded[0]\n\ny_test_gender_index = [np.argmax(i) for i in y_gender_test]\n\ncm = tf.math.confusion_matrix(labels = y_test_gender_index, predictions = prediction_gender_rounded)\n\nplt.figure(figsize = (12,7))\n\nsns.heatmap(cm,annot=True, fmt='d')\n\nplt.xlabel('Prediction')\n\nplt.ylabel('True_value')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n\nprint(classification_report(y_test_gender_index, prediction_gender_rounded))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n\nprint(classification_report(y_test_index, prediction_accent_rounded))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Testing All the models","metadata":{}},{"cell_type":"markdown","source":"prepare test samples","metadata":{}},{"cell_type":"code","source":"test_data = test_data[test_data['file_missing?']==False]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data['native_language'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data[test_data['sex']=='female'].head(20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data['filename'] = test_data['filename'].apply(lambda x: x+'.mp3')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f_female_non_native = ['afrikaans1.mp3']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_path = 'recordings/'\nconfig = Config(sampling_rate=22050, audio_duration=20, learning_rate=0.0001, n_mfcc=13, n_classes=2)\nX_female_non_native = prepare_data(f_female_non_native, config, data_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Play male from Kentucky\nfname_m = 'recordings/' + 'afrikaans1.mp3'\nipd.Audio(fname_m)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"a=encoder.transform(['female'])\ny_female_non_native_gender = to_categorical(np.array(a),  num_classes=2)\nprint(y_female_non_native_gender)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoder1.transform(['non-native'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoder.transform(['female'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Label Encoding:\nFemale - 0\nMale  - 1\n\nNative - 0\nNon-native - 1","metadata":{}},{"cell_type":"code","source":"a=encoder1.transform(['non-native'])\ny_female_non_native_accent = to_categorical(np.array(a),  num_classes=2)\nprint(y_female_non_native_accent)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred = crnn_multi_model.predict(X_female_non_native)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"X=%s, Accent Predicted=%s\" % (pred[0], y_female_non_native_accent[0]))\nprint(\"X=%s, Gender Predicted=%s\" % (pred[1], y_female_non_native_gender[0]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.models import load_model\ncnn_multi_model = load_model('/kaggle/working/saved_models/multitask_classification_cnn.h5')\npred1 = cnn_multi_model.predict(X_female_non_native)\nprint(\"X=%s, Accent Predicted=%s\" % (pred1[0], y_female_non_native_accent[0]))\nprint(\"X=%s, Gender Predicted=%s\" % (pred1[1], y_female_non_native_gender[0]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"African Male accent","metadata":{}},{"cell_type":"code","source":"f_male_non_native = ['afrikaans4.mp3']\nX_male_non_native = prepare_data(f_female_non_native, config, data_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"a=encoder.transform(['male'])\ny_male_non_native_gender = to_categorical(np.array(a),  num_classes=2)\nprint(y_male_non_native_gender)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"a=encoder1.transform(['non-native'])\ny_male_non_native_accent = to_categorical(np.array(a),  num_classes=2)\nprint(y_male_non_native_accent)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.models import load_model\ncnn_multi_model = load_model('/kaggle/working/saved_models/multitask_classification_cnn.h5')\npred = cnn_multi_model.predict(X_male_non_native)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"X=%s, Accent Predicted=%s\" % (pred[0], y_male_non_native_accent[0]))\nprint(\"X=%s, Gender Predicted=%s\" % (pred[1], y_male_non_native_gender[0]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred1 = crnn_multi_model.predict(X_male_non_native)\nprint(\"X=%s, Accent Predicted=%s\" % (pred1[0], y_male_non_native_accent[0]))\nprint(\"X=%s, Gender Predicted=%s\" % (pred1[1], y_male_non_native_gender[0]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"English Male","metadata":{}},{"cell_type":"code","source":"file = '../input/common-voice-samples/English-male.mp3'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_male_native = prepare_data(['English-male.mp3'], config, '/kaggle/input/common-voice-samples')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"a=encoder.transform(['male'])\ny_male_native_gender = to_categorical(np.array(a),  num_classes=2)\nprint(y_male_native_gender)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"a=encoder1.transform(['native'])\ny_male_native_accent = to_categorical(np.array(a),  num_classes=2)\nprint(y_male_native_accent)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred1 = crnn_multi_model.predict(X_male_native)\nprint(\"X=%s, Accent Predicted=%s\" % (pred1[0], y_male_native_accent[0]))\nprint(\"X=%s, Gender Predicted=%s\" % (pred1[1], y_male_native_gender[0]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred1 = cnn_multi_model.predict(X_male_native)\nprint(\"X=%s, Accent Predicted=%s\" % (pred1[0], y_male_native_accent[0]))\nprint(\"X=%s, Gender Predicted=%s\" % (pred1[1], y_male_native_gender[0]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"US female","metadata":{}},{"cell_type":"code","source":"filename = 'kaggle/input/common-voice/cv-other-test/cv-other-test/sample-001204.mp3'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_female_native = prepare_data(['sample-001204.mp3'], config, '/kaggle/input/common-voice/cv-other-dev/cv-other-dev')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"a=encoder.transform(['female'])\ny_female_native_gender = to_categorical(np.array(a),  num_classes=2)\nprint(y_female_native_gender)\na=encoder1.transform(['native'])\ny_female_native_accent = to_categorical(np.array(a),  num_classes=2)\nprint(y_female_native_accent)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred1 = crnn_multi_model.predict(X_female_native)\nprint(\"X=%s, Accent Predicted=%s\" % (pred1[0], y_male_native_accent[0]))\nprint(\"X=%s, Gender Predicted=%s\" % (pred1[1], y_male_native_gender[0]))\n\npred1 = cnn_multi_model.predict(X_female_native)\nprint(\"X=%s, Accent Predicted=%s\" % (pred1[0], y_male_native_accent[0]))\nprint(\"X=%s, Gender Predicted=%s\" % (pred1[1], y_male_native_gender[0]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.models import load_model\ncnn_accent_model = load_model('/kaggle/working/saved_models/accent_classification_cnn.h5')\ncnn_gender_model = load_model('/kaggle/working/saved_models/gender_classification_cnn.h5')\nffn_multi_model = load_model('/kaggle/working/saved_models/audio_classification_ffn.hdf5')\npred1 = cnn_accent_model.predict(X_female_native)\nprint(\"Accent Predicted=%s, X=%s\" % (pred1[0], y_female_native_accent[0]))\npred1 = cnn_accent_model.predict(X_male_native)\nprint(\"Accent Predicted=%s, X=%s\" % (pred1[0], y_male_native_accent[0]))\npred1 = cnn_accent_model.predict(X_male_non_native)\nprint(\"Accent Predicted=%s, X=%s\" % (pred1[0], y_male_non_native_accent[0]))\npred1 = ffn_multi_model.predict(X_female_non_native)\nprint(\"Accent Predicted=%s, X=%s\" % (pred1[0], y_female_non_native_accent[0]))\npred1 = cnn_gender_model.predict(X_female_native)\nprint(\"Accent Predicted=%s, X=%s\" % (pred1[0], y_female_native_gender[0]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.models import load_model\ncrnn_accent_model = load_model('/kaggle/working/saved_models/accent_crnn_classification.h5')\ncrnn_gender_model = load_model('/kaggle/working/saved_models/gender_crnn_classification.h5')\npred1 = crnn_accent_model.predict(X_female_native)\nprint(\"X=%s, Accent Predicted=%s\" % (pred1[0], y_female_native_accent[0]))\npred1 = crnn_gender_model.predict(X_female_native)\nprint(\"X=%s, Gender Predicted=%s\" % (pred1[0], y_female_native_gender[0]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ffn_multi_model = load_model('/kaggle/working/saved_models/audio_classification_ffn.hdf5')\n# Female, Native\npred1 = ffn_multi_model.predict(X_female_native)\nprint(\"X=%s, Accent Predicted=%s\" % (pred1[0], y_female_native_accent[0]))\nprint(\"X=%s, Gender Predicted=%s\" % (pred1[1], y_female_native_gender[0]))\n# Male, Native\npred1 = ffn_multi_model.predict(X_male_native)\nprint(\"X=%s, Accent Predicted=%s\" % (pred1[0], y_male_native_accent[0]))\nprint(\"X=%s, Gender Predicted=%s\" % (pred1[1], y_male_native_gender[0]))\n# Male, Non-native\npred1 = ffn_multi_model.predict(X_male_non_native)\nprint(\"X=%s, Accent Predicted=%s\" % (pred1[0], y_male_non_native_accent[0]))\nprint(\"X=%s, Gender Predicted=%s\" % (pred1[1], y_male_non_native_gender[0]))\n\n# Female, Non-native\npred1 = ffn_multi_model.predict(X_female_non_native)\nprint(\"X=%s, Accent Predicted=%s\" % (pred1[0], y_female_non_native_accent[0]))\nprint(\"X=%s, Gender Predicted=%s\" % (pred1[1], y_female_non_native_gender[0]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Multifeature: MFCC + Chroma","metadata":{}},{"cell_type":"code","source":"def prepare_data_multiple(fnames, config, data_dir):\n    X_mfcc = np.empty(shape=(len(fnames), config.dim[0], config.dim[1], 1))\n    X_chroma = np.empty(shape=(len(fnames), 12, config.dim[1], 1))\n    print(X_chroma.shape)\n    input_length = config.audio_length\n    for i, fname in tqdm_notebook(enumerate(fnames), total=len(fnames)):\n        file_path = os.path.join(data_dir, fname)\n        data, _ = librosa.core.load(file_path, sr=config.sampling_rate, res_type=\"kaiser_fast\")\n\n        # Random offset / Padding\n        if len(data) > input_length:\n            max_offset = len(data) - input_length\n            offset = np.random.randint(max_offset)\n            data = data[offset:(input_length+offset)]\n        else:\n            if input_length > len(data):\n                max_offset = input_length - len(data)\n                offset = np.random.randint(max_offset)\n            else:\n                offset = 0\n            data = np.pad(data, (offset, input_length - len(data) - offset), \"constant\")\n\n        data_mfcc = librosa.feature.mfcc(data, sr=config.sampling_rate, n_mfcc=config.n_mfcc)\n        data_mfcc = np.expand_dims(data_mfcc, axis=-1)\n        X_mfcc[i,] = data_mfcc\n        data = librosa.feature.chroma_cqt(data, sr=config.sampling_rate)\n#         print(data.shape)\n        data = np.expand_dims(data, axis=-1)\n        X_chroma[i,] = data\n    return X_mfcc, X_chroma","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_fnames = np.array(dg['filename'])\ndata_path = 'recordings/'\n\nconfig = Config(sampling_rate=22050, audio_duration=20, learning_rate=0.0001, n_mfcc=12, n_classes=2)\nfeature_file_path = OUTPUT_DIR+'mfcc_features.npy'\n\nX_mfcc, X_chroma = prepare_data_multiple(X_fnames, config, data_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train_mfcc, X_test_mfcc, y_gender_train, y_gender_test, y_accent_train, y_accent_test = train_test_split(X_mfcc, y_gender, y_accent, test_size=0.2, random_state=10)\nprint ('Train set:', X_train_mfcc.shape,  y_gender_train.shape)\nprint ('Test set:', X_test_mfcc.shape,  y_gender_test.shape)\nX_train_chroma, X_test_chroma, _, _, _, _ = train_test_split(X_chroma, y_gender, y_accent, test_size=0.2, random_state=10)\n\nX_train_mfcc, X_val_mfcc, _, _, _, _ = train_test_split(X_train_mfcc, y_gender_train, y_accent_train, test_size=0.15, random_state=10)\nX_train_chroma, X_val_chroma, y_gender_train, y_gender_val, y_accent_train, y_accent_val = train_test_split(X_train_chroma, y_gender_train, y_accent_train, test_size=0.15, random_state=10)\n\nmean = np.mean(X_train_mfcc, axis=0)\nstd = np.std(X_train_mfcc, axis=0)\n\nX_train_mfcc = (X_train_mfcc - mean)/std\nX_val_mfcc = (X_val_mfcc - mean)/std\nX_test_mfcc = (X_test_mfcc - mean)/std\n\nmean = np.mean(X_train_chroma, axis=0)\nstd = np.std(X_train_chroma, axis=0)\n\nX_train_chroma = (X_train_chroma - mean)/std\nX_val_chroma= (X_val_chroma - mean)/std\nX_test_chroma = (X_test_chroma - mean)/std","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_chroma.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_mfcc.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_CRNN_multi_model(input_shape, is_training=True):\n    \n#     model = Sequential()\n    \n#     model.add(Conv2D(64, 3, activation=\"relu\",padding='same', input_shape=input_shape))\n#     model.add(BatchNormalization())\n#     model.add(MaxPooling2D(pool_size=(2, 2)))\n\n#     model.add(Conv2D(128, 3, activation=\"relu\",padding='same'))\n#     model.add(BatchNormalization())\n#     model.add(MaxPooling2D(pool_size=(2, 2)))\n\n#     model.add(Conv2D(128, 3, activation=\"relu\",padding='same'))\n#     model.add(BatchNormalization())\n#     model.add(MaxPooling2D(pool_size=(2, 2)))\n\n#     model.add(Conv2D(256, 3, activation=\"relu\",padding='same'))\n#     model.add(BatchNormalization())\n#     model.add(MaxPooling2D(pool_size=(2, 2)))\n\n#     model.add(Conv2D(512, 3, 3, W_regularizer=l2(weight_decay), activation=\"relu\"))\n#     model.add(BatchNormalization())\n    # model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n\n#     model.add(Convolution2D(512, 3, 3, W_regularizer=l2(weight_decay), activation=\"relu\"))\n#     model.add(BatchNormalization())\n#     model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n\n#     model.add(Conv2D(512, 3, 3, W_regularizer=l2(weight_decay), activation=\"relu\"))\n#     model.add(BatchNormalization())\n#     model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n    \n\n    input_layer_mfcc = Input(shape=(12, 862,1), name='Input')\n    input_layer_chroma = Input(shape=(12, 862,1), name='Input2')\n\n    # conv 1\n    conv_mfcc = Conv2D(32, 3, padding='same', activation='relu')(input_layer_mfcc)\n    batchnorm_mfcc = BatchNormalization()(conv_mfcc)\n    maxpool_mfcc = MaxPooling2D(pool_size=(2, 2))(batchnorm_mfcc)\n    # conv 2\n    conv1_mfcc = Conv2D(32, 3, padding='same', activation='relu')(maxpool_mfcc)\n    batchnorm2_mfcc = BatchNormalization()(conv1_mfcc)\n    maxpool1_mfcc = MaxPooling2D(pool_size=(2, 2))(batchnorm2_mfcc)\n\n    # conv 3\n    conv2_mfcc = Conv2D(32, 3, padding='same', activation='relu')(maxpool1_mfcc)\n    batchnorm4_mfcc = BatchNormalization()(conv2_mfcc)\n    maxpool2_mfcc = MaxPooling2D(pool_size=(2, 2))(batchnorm4_mfcc)\n    \n    # conv 1\n    conv_chroma = Conv2D(32, 3, padding='same', activation='relu')(input_layer_chroma)\n    batchnorm_chroma = BatchNormalization()(conv_chroma)\n    maxpool_chroma = MaxPooling2D(pool_size=(2, 2))(batchnorm_chroma)\n    # conv 2\n    conv1_chroma = Conv2D(32, 3, padding='same', activation='relu')(maxpool_chroma)\n    batchnorm2_chroma = BatchNormalization()(conv1_chroma)\n    maxpool1_chroma = MaxPooling2D(pool_size=(2, 2))(batchnorm2_chroma)\n\n    # conv 3\n    conv2_chroma = Conv2D(32, 3, padding='same', activation='relu')(maxpool1_chroma)\n    batchnorm4_chroma = BatchNormalization()(conv2_chroma)\n    maxpool2_chroma = MaxPooling2D(pool_size=(2, 2))(batchnorm4_chroma)\n    \n\n    \n    concat = Concatenate()([maxpool2_mfcc, maxpool2_chroma])\n#     concat = Concatenate()([batchnorm5_mfcc, batchnorm5_chroma])\n    \n    permute = Permute((2, 1, 3))(concat)\n    reshaped = Reshape((107, 64))(permute)\n\n    lstm = Bidirectional(LSTM(256, return_sequences=False))(reshaped)\n    \n    out_accent = Dense(2, activation='sigmoid',name='output_accent')(lstm)\n    out_gender = Dense(2, activation='sigmoid',name='output_gender')(lstm)\n\n    # (bs, y, x, c) --> (bs, x, y, c)\n#     model.add(Permute((2, 1, 3)))\n\n#     # (bs, x, y, c) --> (bs, x, y * c)\n#     bs, x, y, c = model.layers[-1].output_shape\n#     model.add(Reshape((x, y*c)))\n\n#     model.add(Bidirectional(LSTM(256, return_sequences=False), merge_mode=\"concat\"))\n#     model.add([Dense(2, activation=\"sigmoid\", name='output_accent'),Dense(2, activation=\"sigmoid\", name='output_gender')])\n    model = Model(inputs = [input_layer_mfcc, input_layer_chroma] ,outputs=[out_accent, out_gender])\n\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"multitask_crnn_2 = create_CRNN_multi_model((12,862,1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"multitask_crnn_2.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"multitask_crnn_2.compile(optimizer='Adam',loss={'output_accent':'binary_crossentropy','output_gender': 'binary_crossentropy'}, metrics ={'output_accent': 'accuracy', 'output_gender': 'accuracy'})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Training my model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom datetime import datetime \n\nnum_epochs = 35\nnum_batch_size = 32\n\ncheckpointer = ModelCheckpoint(filepath=OUTPUT_DIR + 'saved_models/multitask_multiinput_classification_crnn.h5', \n                               verbose=1, save_best_only=True,patience=10, monitor='val_loss', mode='min')\nearly_stopping = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=5)\n\nstart = datetime.now()\n\nhistory_multitask_crnn2 = multitask_crnn_2.fit({\"Input\": X_train_mfcc, \"Input2\": X_train_chroma}, {\"output_accent\": y_accent_train, \"output_gender\": y_gender_train},validation_data=({\"Input\": X_val_mfcc, \"Input2\": X_val_chroma},{\"output_accent\": y_accent_val, \"output_gender\": y_gender_val}),\n                        batch_size=num_batch_size, epochs=num_epochs, callbacks=[checkpointer, early_stopping], verbose=1)\n\n\nduration = datetime.now() - start\nprint(\"Training completed in time: \", duration)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.models import load_model\ncrnn_multi_model = load_model('/kaggle/working/saved_models/multitask_multiinput_classification_crnn.h5')\nresults_multitask_crnn = crnn_multi_model.evaluate({\"Input\": X_test_mfcc, \"Input2\": X_test_chroma}, {\"output_accent\": y_accent_test, \"output_gender\": y_gender_test})\nprint(results_multitask_crnn)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"FFFN","metadata":{}},{"cell_type":"code","source":"def ffn_multiinputs():\n#     input_layer_mfcc = Input(shape=(12, 862), name='Input')\n#     input_layer_chroma = Input(shape=(12, 862), name='Input2')\n#     concat = Concatenate()([input_layer_mfcc, input_layer_chroma])\n\n#     dense1=Dense(128, activation='relu',name='Hidden_1')(concat)\n#     dropout1 = Dropout(0.5)(dense1)\n\n#     dense2=Dense(256, activation='relu',name='Hidden_2')(dropout1)\n#     dropout2 = Dropout(0.5)(dense2)\n\n#     dense3=Dense(128, activation='relu',name='Hidden_3')(dropout2)\n#     dropout3 = Dropout(0.5)(dense3)\n#     m = tf.keras.layers.Flatten()(dropout3)\n\n#     out_accent = Dense(2, activation='sigmoid',name='output_accent')(m)\n#     out_gender = Dense(2, activation='sigmoid',name='output_gender')(m)\n\n\n#     model_ffn = Model(inputs = [input_layer_mfcc,input_layer_chroma] ,outputs=[out_accent, out_gender])\n#     return model_ffn\n    input_layer_mfcc = Input(shape=(12, 862), name='Input')\n    input_layer_chroma = Input(shape=(12, 862), name='Input2')\n    \n\n    dense1=Dense(128, activation='relu',name='Hidden_1')(input_layer_mfcc)\n    dropout1 = Dropout(0.5)(dense1)\n\n    dense2=Dense(256, activation='relu',name='Hidden_2')(dropout1)\n    dropout2 = Dropout(0.5)(dense2)\n\n    dense3=Dense(128, activation='relu',name='Hidden_3')(dropout2)\n    dropout3 = Dropout(0.5)(dense3)\n    \n    dense1_chroma=Dense(128, activation='relu',name='Hidden_11')(input_layer_chroma)\n    dropout1_chroma = Dropout(0.5)(dense1_chroma)\n\n    dense2_chroma=Dense(256, activation='relu',name='Hidden_21')(dropout1_chroma)\n    dropout2_chroma = Dropout(0.5)(dense2_chroma)\n\n    dense3_chroma=Dense(128, activation='relu',name='Hidden_31')(dropout2_chroma)\n    dropout3_chroma = Dropout(0.5)(dense3_chroma)\n    concat = Concatenate()([dropout3, dropout3_chroma])\n    m = tf.keras.layers.Flatten()(concat)\n\n    out_accent = Dense(2, activation='sigmoid',name='output_accent')(m)\n    out_gender = Dense(2, activation='sigmoid',name='output_gender')(m)\n\n\n    model_ffn = Model(inputs = [input_layer_mfcc,input_layer_chroma] ,outputs=[out_accent, out_gender])\n    return model_ffn","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_ffn = ffn_multiinputs()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_ffn.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_ffn.compile(optimizer='Adam',loss={'output_accent':'binary_crossentropy','output_gender': 'binary_crossentropy'}, metrics ={'output_accent': 'accuracy', 'output_gender': 'accuracy'})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Training my model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom datetime import datetime \n\nnum_epochs = 35\nnum_batch_size = 32\n\ncheckpointer = ModelCheckpoint(filepath=OUTPUT_DIR + 'saved_models/multitask_multiinput_classification_ffn.h5', \n                               verbose=1, save_best_only=True,patience=10, monitor='val_loss', mode='min')\nearly_stopping = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=5)\n\nstart = datetime.now()\n\nhistory_multitask_crnn2 = model_ffn.fit({\"Input\": X_train_mfcc, \"Input2\": X_train_chroma}, {\"output_accent\": y_accent_train, \"output_gender\": y_gender_train},validation_data=({\"Input\": X_val_mfcc, \"Input2\": X_val_chroma},{\"output_accent\": y_accent_val, \"output_gender\": y_gender_val}),\n                        batch_size=num_batch_size, epochs=num_epochs, callbacks=[checkpointer, early_stopping], verbose=1)\n\n\nduration = datetime.now() - start\nprint(\"Training completed in time: \", duration)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.models import load_model\ncrnn_multi_model = load_model('/kaggle/working/saved_models/multitask_multiinput_classification_ffn.h5')\nresults_multitask_crnn = crnn_multi_model.evaluate({\"Input\": X_test_mfcc, \"Input2\": X_test_chroma}, {\"output_accent\": y_accent_test, \"output_gender\": y_gender_test})\nprint(results_multitask_crnn)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### CNN with multi inputs","metadata":{}},{"cell_type":"code","source":"def create_cnn_model2():\n    # Create CNN model\n    input_layer_mfcc = Input(shape=(12, 862,1), name='Input')\n    input_layer_chroma = Input(shape=(12, 862,1), name='Input2')\n    # conv 1\n    conv = Conv2D(16, 3, padding='same', activation='relu')(input_layer_mfcc)\n    batchnorm = BatchNormalization()(conv)\n    maxpool = MaxPooling2D(pool_size=(2, 2))(batchnorm)\n    batchnorm1 = BatchNormalization()(maxpool)\n    # conv 2\n    conv1 = Conv2D(16, 3, padding='same', activation='relu')(batchnorm1)\n    batchnorm2 = BatchNormalization()(conv1)\n    maxpool1 = MaxPooling2D(pool_size=(2, 2))(batchnorm1)\n    batchnorm3 = BatchNormalization()(maxpool1)\n    # conv 3\n    conv2 = Conv2D(128, 3, padding='same', activation='relu')(batchnorm3)\n    batchnorm4 = BatchNormalization()(conv2)\n    maxpool2 = MaxPooling2D(pool_size=(2, 2))(batchnorm4)\n    batchnorm5 = BatchNormalization()(maxpool2)\n    \n    conv_chroma = Conv2D(16, 3, padding='same', activation='relu')(input_layer_chroma)\n    batchnorm_chroma = BatchNormalization()(conv_chroma)\n    maxpool_chroma = MaxPooling2D(pool_size=(2, 2))(batchnorm_chroma)\n    batchnorm1_chroma = BatchNormalization()(maxpool_chroma)\n    # conv 2\n    conv1_chroma = Conv2D(16, 3, padding='same', activation='relu')(batchnorm1_chroma)\n    batchnorm2_chroma = BatchNormalization()(conv1_chroma)\n    maxpool1_chroma = MaxPooling2D(pool_size=(2, 2))(batchnorm1_chroma)\n    batchnorm3_chroma = BatchNormalization()(maxpool1_chroma)\n    # conv 3\n    conv2_chroma = Conv2D(128, 3, padding='same', activation='relu')(batchnorm3_chroma)\n    batchnorm4_chroma = BatchNormalization()(conv2_chroma)\n    maxpool2_chroma = MaxPooling2D(pool_size=(2, 2))(batchnorm4_chroma)\n    batchnorm5_chroma = BatchNormalization()(maxpool2_chroma)\n    \n    concat = Concatenate()([batchnorm5, batchnorm5_chroma])\n    \n    # flatten\n    flatten = Flatten()(concat)\n#     concat = Concatenate()([input_layer_mfcc, input_layer_chroma])\n#     dense = Dense(64, activation='relu',name='Hidden_1')(flatten)\n#     dropout = Dropout(0.5)(dense)\n    # output\n    out_accent = Dense(2, activation='sigmoid',name='output_accent')(flatten)\n    out_gender = Dense(2, activation='sigmoid',name='output_gender')(flatten)\n    model = Model(inputs = [input_layer_mfcc,input_layer_chroma] ,outputs=[out_accent, out_gender])\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"multinput_cnn_multi = create_cnn_model2()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"multinput_cnn_multi.compile(optimizer='Adam',loss={'output_accent':'binary_crossentropy','output_gender': 'binary_crossentropy'}, metrics ={'output_accent': 'accuracy', 'output_gender': 'accuracy'})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Training my model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom datetime import datetime \n\nnum_epochs = 35\nnum_batch_size = 32\n\ncheckpointer = ModelCheckpoint(filepath=OUTPUT_DIR + 'saved_models/multitask_multiinput_classification_cnn.h5', \n                               verbose=1, save_best_only=True,patience=10, monitor='val_loss', mode='min')\nearly_stopping = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=5)\n\nstart = datetime.now()\n\nhistory_multinput_cnn_multi = multinput_cnn_multi.fit({\"Input\": X_train_mfcc, \"Input2\": X_train_chroma}, {\"output_accent\": y_accent_train, \"output_gender\": y_gender_train},validation_data=({\"Input\": X_val_mfcc, \"Input2\": X_val_chroma},{\"output_accent\": y_accent_val, \"output_gender\": y_gender_val}),\n                        batch_size=num_batch_size, epochs=num_epochs, callbacks=[checkpointer, early_stopping], verbose=1)\n\n\nduration = datetime.now() - start\nprint(\"Training completed in time: \", duration)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.models import load_model\ncrnn_multi_model = load_model('/kaggle/working/saved_models/multitask_multiinput_classification_cnn.h5')\nresults_multitask_crnn = crnn_multi_model.evaluate({\"Input\": X_test_mfcc, \"Input2\": X_test_chroma}, {\"output_accent\": y_accent_test, \"output_gender\": y_gender_test})\nprint(results_multitask_crnn)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_cnn_model2():\n    # Create CNN model\n#     input_layer_mfcc = Input(shape=(12, 862,1), name='Input')\n    input_layer_chroma = Input(shape=(12, 862,1), name='Input2')\n#     # conv 1\n#     conv = Conv2D(32, 3, padding='same', activation='relu')(input_layer_mfcc)\n#     batchnorm = BatchNormalization()(conv)\n#     maxpool = MaxPooling2D(pool_size=(2, 2))(batchnorm)\n#     batchnorm1 = BatchNormalization()(maxpool)\n#     # conv 2\n#     conv1 = Conv2D(32, 3, padding='same', activation='relu')(batchnorm1)\n#     batchnorm2 = BatchNormalization()(conv1)\n#     maxpool1 = MaxPooling2D(pool_size=(2, 2))(batchnorm1)\n#     batchnorm3 = BatchNormalization()(maxpool1)\n#     # conv 3\n#     conv2 = Conv2D(128, 3, padding='same', activation='relu')(batchnorm3)\n#     batchnorm4 = BatchNormalization()(conv2)\n#     maxpool2 = MaxPooling2D(pool_size=(2, 2))(batchnorm4)\n#     batchnorm5 = BatchNormalization()(maxpool2)\n    \n    conv_chroma = Conv2D(32, 3, padding='same', activation='relu')(input_layer_chroma)\n    batchnorm_chroma = BatchNormalization()(conv_chroma)\n    maxpool_chroma = MaxPooling2D(pool_size=(2, 2))(batchnorm_chroma)\n    batchnorm1_chroma = BatchNormalization()(maxpool_chroma)\n    # conv 2\n    conv1_chroma = Conv2D(64, 3, padding='same', activation='relu')(batchnorm1_chroma)\n    batchnorm2_chroma = BatchNormalization()(conv1_chroma)\n    maxpool1_chroma = MaxPooling2D(pool_size=(2, 2))(batchnorm1_chroma)\n    batchnorm3_chroma = BatchNormalization()(maxpool1_chroma)\n    # conv 3\n    conv2_chroma = Conv2D(128, 3, padding='same', activation='relu')(batchnorm3_chroma)\n    batchnorm4_chroma = BatchNormalization()(conv2_chroma)\n    maxpool2_chroma = MaxPooling2D(pool_size=(2, 2))(batchnorm4_chroma)\n    batchnorm5_chroma = BatchNormalization()(maxpool2_chroma)\n    \n#     concat = Concatenate()([batchnorm5, batchnorm5_chroma])\n    \n    # flatten\n    flatten = Flatten()(batchnorm5_chroma)\n#     concat = Concatenate()([input_layer_mfcc, input_layer_chroma])\n    dense = Dense(64, activation='relu',name='Hidden_1')(flatten)\n    dropout = Dropout(0.5)(dense)\n    # output\n    out = Dense(2, activation='sigmoid',name='output')(dropout)\n    model = Model(inputs = input_layer_chroma ,outputs=out)\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"multinput_cnn_accent = create_cnn_model2()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"multinput_cnn_accent.compile(optimizer='Adam',loss='binary_crossentropy', metrics =['accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Training my model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom datetime import datetime \n\nnum_epochs = 35\nnum_batch_size = 32\n\ncheckpointer = ModelCheckpoint(filepath=OUTPUT_DIR + 'saved_models/multitask_multiinput_classification_ffn.h5', \n                               verbose=1, save_best_only=True,patience=10, monitor='val_loss', mode='min')\nearly_stopping = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=5)\n\nstart = datetime.now()\n\nhistory_multitask_crnn2 = multinput_cnn_accent.fit({\"Input\": X_train_mfcc, \"Input2\": X_train_chroma}, y_accent_train,validation_data=({\"Input\": X_val_mfcc, \"Input2\": X_val_chroma},y_accent_val),batch_size=num_batch_size, epochs=num_epochs, callbacks=[checkpointer, early_stopping], verbose=1)\n\n\nduration = datetime.now() - start\nprint(\"Training completed in time: \", duration)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}